# -*- coding: utf-8 -*-
"""SearchAndWebscraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rI_NtZncc4jHpV5YVuKajZZ-QWeu6mmq
"""

from googlesearch import search
print("Googlesearch package installed successfully!")

# set query to search for in Google
query = "long winter coat"
# execute query and store search results
results = search(query, tld="com", lang="en", stop=3, pause=2)
# iterate over all search results and print them
for result in results:
    print(result)

# set query to search for in Google
query = input("Enter: ")
# execute query and store search results
results = search(query, tld="com", lang="en", stop=5, pause=2)
# iterate over all search results and print them
for result in results:
    print(result)

# set query to search for in Google
query = "dengue indiatoday"
# execute query and store search results
# stop = int(input("Enter no. of links reqd.: "))
results = search(query, tld="com", lang="en", stop = int(input("Enter no. of links reqd.: ")), pause=2)
# iterate over all search results and print them
for result in results:
    print(result)

import urllib.request

webUrl=urllib.request.urlopen('https://www.python.org/')

print("result: "+str(webUrl.getCode()))

import webbrowser

url = 'https://codefather.tech/'
webbrowser.open(url)

import webbrowser

url = 'https://codefather.tech/'

# Open in new tab
webbrowser.open_new_tab(url)

# Open in new window
webbrowser.open_new(url)

import webbrowser

url = 'https://codefather.tech/'

# Open in Chrome browser
chrome = webbrowser.get('chrome')
chrome.open_new_tab(url)

import webbrowser

urls = [
    'https://codefather.tech/',
    'https://www.google.com/',
    'https://github.com/',
]

browser = webbrowser.get()

for url in urls:
    browser.open_new_tab(url)

import requests

from bs4 import BeautifulSoup
import pandas as pd
import time

# URL of the website to scrape
url = "https://www.imdb.com/chart/top"

# Send an HTTP GET request to the website
response = requests.get(url)

# Parse the HTML code using BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Extract the relevant information from the HTML code
movies = []
for row in soup.select('tbody.lister-list tr'):
    title = row.find('td', class_='titleColumn').find('a').get_text()
    year = row.find('td', class_='titleColumn').find('span', class_='secondaryInfo').get_text()[1:-1]
    rating = row.find('td', class_='ratingColumn imdbRating').find('strong').get_text()
    movies.append([title, year, rating])
print(movies)
# Store the information in a pandas dataframe
df = pd.DataFrame(movies, columns=['Title', 'Year', 'Rating'])

# Add a delay between requests to avoid overwhelming the website with requests
time.sleep(1)

# Export the data to a CSV file
df.to_csv('top-rated-movies.csv', index=False)

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the URL of the webpage you want to access
response = requests.get("https://www.example.com")

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, "html.parser")

# Extract the text content of the webpage
text = soup.get_text()

print(text)

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the URL of the webpage
# req = requests.get("https://www.hindustantimes.com/topic/dengue")
req = requests.get("https://www.indiatoday.in/health/story/how-rising-dengue-burden-is-costing-india-and-sri-lanka-billions-2516716-2024-03-19")

# Parse the HTML content
soup = BeautifulSoup(req.text, "html.parser")

# Find all instances of the word "blood"
# topics = soup.find_all("p", text=lambda x: "mosquito" in x.string.lower())

if soup.find('p'):
    topics = soup.find_all("p", text=lambda x: "mosquito" in x.string.lower())
else:
    print("No <p> tags found in the HTML content.")

# Display the results
# print(topics)
for p in soup.find_all('p'):
    print(p.text)
# for topic in topics:
#     print(topic.string)

import requests
from bs4 import BeautifulSoup

# Send an HTTP request to the URL of the webpage you want to access
# response = requests.get("https://www.hindustantimes.com/topic/dengue")
response = requests.get("https://www.indiatoday.in/health/story/how-rising-dengue-burden-is-costing-india-and-sri-lanka-billions-2516716-2024-03-19")

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.content, "html.parser")

# Extract the text content of the webpage
text = soup.get_text()

print(text)

# https://realpython.com/python-web-scraping-practical-introduction/#:~:text=One%20way%20to%20extract%20information,requested%20in%20the%20previous%20example.

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from heapq import nlargest

def summarize(text, per):
    nlp = spacy.load('en_core_web_sm')
    doc= nlp(text)
    tokens=[token.text for token in doc]
    word_frequencies={}
    for word in doc:
        if word.text.lower() not in list(STOP_WORDS):
            if word.text.lower() not in punctuation:
                if word.text not in word_frequencies.keys():
                    word_frequencies[word.text] = 1
                else:
                    word_frequencies[word.text] += 1
    max_frequency=max(word_frequencies.values())
    for word in word_frequencies.keys():
        word_frequencies[word]=word_frequencies[word]/max_frequency
    sentence_tokens= [sent for sent in doc.sents]
    sentence_scores = {}
    for sent in sentence_tokens:
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if sent not in sentence_scores.keys():
                    sentence_scores[sent]=word_frequencies[word.text.lower()]
                else:
                    sentence_scores[sent]+=word_frequencies[word.text.lower()]
    select_length=int(len(sentence_tokens)*per)
    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)
    final_summary=[word.text for word in summary]
    summary=''.join(final_summary)
    return summary

summarize(article.text, 0.05)